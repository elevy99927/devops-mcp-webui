services:
  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    volumes:
      - ./ollama/models:/root/.ollama/models # Persist models
    ports:
      - "11434:11434" # Ollama API port
    restart: always
    networks:
      - mcp-lab-network
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    # For GPU support, uncomment the following lines and ensure your system has NVIDIA drivers and Docker configured for GPU access
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]


  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:latest
    depends_on:
      - mcpo
      - ollama
    ports:
      - "3000:8080" # Map host port 3000 to container port 8080
    volumes:
      - ./open-webui/data:/app/backend/data
      - ./functions:/app/backend/functions:ro
    environment:
      - ENABLE_OLLAMA_API=true
      - OLLAMA_BASE_URL=http://ollama:11434
      - ENABLE_OPENAI_API=true
      - ENABLE_FUNCTION_CALLING=true
      - GLOBAL_LOG_LEVEL=DEBUG
      # Disable features that might cause auth issues
      - ENABLE_COMMUNITY_SHARING=false
      - ENABLE_MESSAGE_RATING=false
      - ENABLE_ADMIN_EXPORT=true
      # MCP Configuration
      - MCPO_URL=http://mcpo:9000
      - OPENWEBUI_MCP_SERVER_URLS=http://mcpo:9000
      # Authentication Settings
#      - WEBUI_AUTH=false
#      - ENABLE_SIGNUP=false
#      - DEFAULT_USER_ROLE=admin
#      - WEBUI_SECRET_KEY=rpg-secret-key-123
#      - WEBUI_NAME="RPG Interface"
#      - WEBUI_URL=http://localhost:3000
    networks:
      - mcp-lab-network
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  k8s-mcp-server-backend:
    image: ghcr.io/alexei-led/k8s-mcp-server:latest
    container_name: k8s-mcp-server-backend
    networks:
      - mcp-lab-network
    restart: unless-stopped
    volumes:
      - ./kube:/home/appuser/.kube:ro
    environment:
      - KUBECONFIG=/home/appuser/.kube/config
      - K8S_MCP_TRANSPORT=sse
      - K8S_MCP_SECURITY_MODE=permissive
      - K8S_MCP_TIMEOUT=600
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  mcpo:
    build:
      context: ./mcp-bridge
    container_name: mcpo
    ports:
      - "9000:9000"
    networks:
      - mcp-lab-network
      - kind
    restart: unless-stopped
    depends_on:
      - k8s-mcp-server-backend
    volumes:
      - ./kube:/root/.kube:ro
    environment:
      - KUBECONFIG=/root/.kube/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"




networks:
  mcp-lab-network:
    driver: bridge
  kind:
    external: true

volumes:
  kind-data:
